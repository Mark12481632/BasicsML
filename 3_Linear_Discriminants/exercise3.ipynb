{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BasicsML - Exercise 3: Linear Discriminants\n",
    "In this exercise, you will implement (generalized) linear discriminant models to classify some toy data.\n",
    "\n",
    "Make sure to replace all parts that say\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Least Squares Linear Classifier\n",
    "We start off with a simple least-squares classifier.\n",
    "This model learns the weights and bias of a linear discriminant function $y(\\mathbf{x})$:\n",
    "\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + w_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a least-squares linear classifier on synthetic 2D training data and evaluate it on the training and test set.\n",
    "You should have extracted the files containing the training and test data together with this notebook.\n",
    "\n",
    "We already implemented the dataloading, plotting, and evaluation code. Feel free to take a look at them before continuing with the exercise further below.\n",
    "You will need to write two functions:\n",
    "- `leastSquares` trains a linear discriminant using the least-squares objective.\n",
    "- `linclass` applies a linear classifier on some data and returns the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    data = np.load(f\"{name}.npz\")\n",
    "    return {s: data[s] for s in (\"data\", \"labels\")}\n",
    "\n",
    "\n",
    "def plot_contour(fun):\n",
    "    # make a regular grid over the whole plot\n",
    "    x, y = np.linspace(*plt.xlim(), 200), np.linspace(*plt.ylim(), 200)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # evaluate function on grid\n",
    "    res = fun(grid).reshape(*xx.shape)\n",
    "\n",
    "    # plot contour lines (=decision boundary) and filled contours over whole plot\n",
    "    plt.contourf(xx, yy, res, colors=[\"yellow\", \"blue\"], alpha=0.2)\n",
    "    plt.contour(xx, yy, res, levels=1, colors=\"k\")\n",
    "\n",
    "\n",
    "def plot_(data, labels, params=None, title=None, basis_fun=None):\n",
    "    # Plot the data points and the decision line\n",
    "    plt.subplot()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    class1, class2 = data[labels > 0], data[labels < 0]\n",
    "    plt.scatter(*class1.T, c=\"blue\", marker=\"x\")\n",
    "    plt.scatter(*class2.T, c=\"orange\", marker=\"o\")\n",
    "\n",
    "    if params:\n",
    "        w, b = params\n",
    "        xmax = data[:, 0].max(0)\n",
    "        xmin = data[:, 0].min(0)\n",
    "        # Quick and hacky way to fix the y-axis limits\n",
    "        plt.ylim(plt.ylim())\n",
    "\n",
    "        if basis_fun:\n",
    "            # evaluate decision function on grid and plot contours\n",
    "            plot_contour(lambda grid: linclass(w, b, basis_fun(grid)))\n",
    "        else:\n",
    "            # just plot a line\n",
    "            y = lambda x: -(w[0] * x + b) / w[1]\n",
    "            plt.plot([xmin, xmax], [y(xmin), y(xmax)], c=\"k\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1\n",
    "Implement the `leastSquares` function.\n",
    "It should train a least-squares classifier based on the data matrix $\\mathbf{X}$ and its class label vector $\\mathbf{t}$.\n",
    "As output, it produces the linear classifier weight vector $\\mathbf{w}$ and bias $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd8cffb44773f23fecaec32e7f0cda74",
     "grade": true,
     "grade_id": "q1a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def leastSquares(data, label):\n",
    "    # Minimize the sum-of-squares error\n",
    "    #\n",
    "    # INPUT:\n",
    "    # data        : Training inputs  (num_samples x dim)\n",
    "    # label       : Training targets- 1D array of length {num_samples}\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # weights     : weights- 1D array of length {dim}\n",
    "    # bias        : bias term (scalar)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2\n",
    "Implement the function `linclass` that classifies a data matrix $\\mathbf{X}$ based on a trained linear classifier given by $\\mathbf{w}$ and $b$.\n",
    "Remember that we expect classification results to be either 1 or -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linclass(weight, bias, data):\n",
    "    # Apply a linear classifier\n",
    "    #\n",
    "    # INPUT:\n",
    "    # weight      : weights                1D array of length {dim}\n",
    "    # bias        : bias term              (scalar)\n",
    "    # data        : Input to be classified (num_samples x dim)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # class_pred       : Predicted class (+-1) values- 1D array of length {num_samples}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3\n",
    "Run the cells below to train and evaluate a linear classifier on the provided data.\n",
    "Analyze the classification plots for both the datasets. Are the sets optimally classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data(\"lc_train\")\n",
    "test = load_data(\"lc_test\")\n",
    "\n",
    "\n",
    "def evaluate(data, label, params, title, basis_fun=None):\n",
    "    pred = linclass(weight, bias, basis_fun(data) if basis_fun is not None else data)\n",
    "    acc = (pred == label).mean()\n",
    "    print(f\"Accuracy on {title}: {acc:.5f}\")\n",
    "    plot_(data, label, params, title, basis_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = leastSquares(train[\"data\"], train[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train[\"data\"], train[\"labels\"], (weight, bias), \"Train Set\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test[\"data\"], test[\"labels\"], (weight, bias), \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4\n",
    "Now we add some outliers to the data.\n",
    "Run the cells below to train and evaluate the classifier on the augmented data.\n",
    "\n",
    "Again, analyze the classification plots for both datasets. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_train = {\n",
    "    \"data\": np.append(train[\"data\"], [[1.5, -0.4], [1.45, -0.35]], axis=0),\n",
    "    \"labels\": np.append(train[\"labels\"], [[-1], [-1]]),\n",
    "}\n",
    "\n",
    "weight, bias = leastSquares(outlier_train[\"data\"], outlier_train[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(\n",
    "    outlier_train[\"data\"],\n",
    "    outlier_train[\"labels\"],\n",
    "    (weight, bias),\n",
    "    \"Train Set with Outliers\",\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test[\"data\"], test[\"labels\"], (weight, bias), \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Basis Functions\n",
    "Now we will implement polynomial basis functions and use them to classifiy non-linearly separable data.\n",
    "Our basis functions will have the following form:\n",
    "\n",
    "To start, implement $\\phi_2(\\mathbf{x})$, that is, a second degree polynomial basis function, for two-dimensional data:\n",
    "\n",
    "$$\n",
    "\\phi_2(\\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_two(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "w = np.array([0.25, -0.5, -0.5, 0, 0, 1])\n",
    "plot_contour(lambda x: linclass(w, 0, poly_two(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a linear discriminant using this basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poly = poly_two(outlier_train[\"data\"])\n",
    "test_poly = poly_two(test[\"data\"])\n",
    "\n",
    "weight, bias = leastSquares(train_poly, outlier_train[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(\n",
    "    outlier_train[\"data\"],\n",
    "    outlier_train[\"labels\"],\n",
    "    (weight, bias),\n",
    "    \"Train Set\",\n",
    "    poly_two,\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test[\"data\"], test[\"labels\"], (weight, bias), \"Test Set\", poly_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play around, you may also implement a general polynomial basis function for arbitrary degrees.\n",
    "What do you notice when increasing the degree of the polynomial?\n",
    "What effect do outliers have on the classification result and the decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Generalized Linear Discriminants and Iterative Optimization\n",
    "In order to limit the influence of outliers, we will now add activation functions to our discriminants:\n",
    "    \n",
    "$$\n",
    "y(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}) + b)\n",
    "$$\n",
    "\n",
    "Since there is in general no closed-form solution to the training objective any more, we will use gradient descent to optimize it iteratively.\n",
    "\n",
    "In order to do this, you need to implement the following functions:\n",
    "- the activation function itself (we will use the tangens hyperbolicus discussed in the lecture)\n",
    "- the derivative of the activation function\n",
    "- the gradient descent update equation\n",
    "\n",
    "The rest is provided by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1: Implement Tanh\n",
    "Implement the tanh function as\n",
    "\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "Show that the derivative of $\\text{tanh}$ is\n",
    "\n",
    "$$\n",
    "\\text{tanh}^\\prime(x) = 1 - \\text{tanh}(x)^2\n",
    "$$\n",
    "\n",
    "Implement the `tanh` and `grad_tanh` functions below; they should apply $\\text{tanh}$ (and $\\text{tanh}^\\prime$, respectively) element-wise to the input.\n",
    "Check the resulting plot to see whether your implementation makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def grad_tanh(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def plot__(*fs):\n",
    "    x = np.linspace(-5, 5, 400)\n",
    "    for f in fs:\n",
    "        plt.plot(x, f(x), label=f.__name__)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot__(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we implemented a method to check your analytical derivative against a numerical approximation, using\n",
    "\n",
    "$$\n",
    "f^\\prime(x) = \\lim_{\\epsilon\\rightarrow0} \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "You can run it to test whether your derivative matches your activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(f, f_grad, eps=1e-5):\n",
    "    x = np.random.randn(100)\n",
    "    num_grad = (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "    if np.allclose(num_grad, f_grad(x)):\n",
    "        print(f\"Grad for {f.__name__} seems correct\")\n",
    "    else:\n",
    "        print(f\"Numeric and analytical grad for {f.__name__} differ:\")\n",
    "        print(num_grad - f_grad(x))\n",
    "\n",
    "\n",
    "check_grad(tanh, grad_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2: Implement Gradient Descent\n",
    "Implement the missing parts of the gradient descent algorithm below.\n",
    "Run the code to train a model on the data that we provided.\n",
    "\n",
    "To do this, you should first calculate the error gradient with respect to the learnable parameters, then use the gradient to update these parameters.\n",
    "Remember the formula for gradient descent from the lecture:\n",
    "\n",
    "$$\n",
    "w^{(\\tau+1)} = w^{(\\tau)} - \\eta \\nabla E(w^{(\\tau)}),\n",
    "$$\n",
    "\n",
    "using the squared error function and a generalized linear discriminant:\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{n=1}^N (\\sigma(w^T \\phi(x_n)) - t_n)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla E = \\sum_{n=1}^N (y_n - t_n) \\sigma^{\\prime}(w^T \\phi(x_n)) \\phi(x_n).\n",
    "$$\n",
    "\n",
    "Use the `tanh` and `grad_tanh` functions that you implemented above.\n",
    "\n",
    "\n",
    "What results do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(eta, iters, data, label):\n",
    "    N = len(data)\n",
    "    data = np.concatenate((np.ones((N, 1)), data), axis=1)\n",
    "    w = np.random.randn(data.shape[1])\n",
    "\n",
    "    errors = []\n",
    "    for i in range(iters):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    plt.plot(errors)\n",
    "    plt.title(\"Error over gradient descent iterations\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "\n",
    "    # first entry is bias term\n",
    "    return w[1:], w[0]\n",
    "\n",
    "eta = 0.01\n",
    "iters = 1000\n",
    "\n",
    "weight, bias = grad_descent(eta, iters, outlier_train[\"data\"], outlier_train[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(outlier_train[\"data\"], outlier_train[\"labels\"], (weight, bias), \"Train Set\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test[\"data\"], test[\"labels\"], (weight, bias), \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on some different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different dataset\n",
    "train_XOR = load_data(\"XOR_train\")\n",
    "test_XOR = load_data(\"XOR_test\")\n",
    "\n",
    "weight, bias = grad_descent(eta, iters, train_XOR[\"data\"], train_XOR[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train_XOR[\"data\"], train_XOR[\"labels\"], (weight, bias), \"Train Set\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test_XOR[\"data\"], test_XOR[\"labels\"], (weight, bias), \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous outcome is unsatisfactory due to the XOR dataset's lack of linear separability. Therefore, we can attempt to incorporate a kernel function to allow our classifier to establish a nonlinear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"try poly_two kernel\"\"\"\n",
    "\n",
    "# try different dataset\n",
    "train_XOR = load_data(\"XOR_train\")\n",
    "test_XOR = load_data(\"XOR_test\")\n",
    "\n",
    "weight, bias = grad_descent(eta, iters, poly_two(train_XOR[\"data\"]), train_XOR[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train_XOR[\"data\"], train_XOR[\"labels\"], (weight, bias), \"Train Set\", poly_two)\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test_XOR[\"data\"], test_XOR[\"labels\"], (weight, bias), \"Test Set\", poly_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different dataset\n",
    "train_concentric = load_data(\"Concentric_train\")\n",
    "test_concentric = load_data(\"Concentric_test\")\n",
    "\n",
    "weight, bias = grad_descent(eta, iters, poly_two(train_concentric[\"data\"]), train_concentric[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train_concentric[\"data\"], train_concentric[\"labels\"], (weight, bias), \"Train Set\", poly_two)\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test_concentric[\"data\"], test_concentric[\"labels\"], (weight, bias), \"Test Set\", poly_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different dataset\n",
    "train_CC = load_data(\"CC_train\")\n",
    "test_CC = load_data(\"CC_test\")\n",
    "\n",
    "weight, bias = grad_descent(eta, iters, poly_two(train_CC[\"data\"]), train_CC[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train_CC[\"data\"], train_CC[\"labels\"], (weight, bias), \"Train Set\", poly_two)\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test_CC[\"data\"], test_CC[\"labels\"], (weight, bias), \"Test Set\", poly_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.3: Enhancing the polynomial kernel\n",
    "The previous outcome does not meet our expectations as we have only utilized a 2nd-degree polynomial kernel. To improve the results, let's consider using a higher-order polynomial kernel!\n",
    "\n",
    "Please complete the missing section in the poly function below. Afterward, execute the code to train a model using the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "train_CC = load_data(\"CC_train\")\n",
    "test_CC = load_data(\"CC_test\")\n",
    "\n",
    "eta = 0.01\n",
    "iters = 10000\n",
    "weight, bias = grad_descent(eta, iters, poly(train_CC[\"data\"]), train_CC[\"labels\"])\n",
    "\n",
    "# Evaluate on the train set\n",
    "evaluate(train_CC[\"data\"], train_CC[\"labels\"], (weight, bias), \"Train Set\", poly)\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate(test_CC[\"data\"], test_CC[\"labels\"], (weight, bias), \"Test Set\", poly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
