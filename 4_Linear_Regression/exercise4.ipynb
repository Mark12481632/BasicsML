{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78136b6",
   "metadata": {},
   "source": [
    "![BridgingAI Logo](../bridgingai_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd037773",
   "metadata": {},
   "source": [
    "# BasicsML - Exercise 4: Linear Regression\n",
    "In this exercise, you will implement linear regression using the least-squares formulation as well as ridge regression for improved robustness against overfitting.\n",
    "\n",
    "Make sure to replace all parts that say\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92ac44",
   "metadata": {},
   "source": [
    "# Q1: Implement least squares regression\n",
    "In this first part of the exercise, you implement the least-squares solution to linear regression.\n",
    "\n",
    "**Hint:** this is very similar to training a least-squares classifier, so you might be able to reuse code from that exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558280d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198bcdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    data = np.load(f\"{name}.npz\")\n",
    "    return tuple(data[f] for f in data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_(data, label, params=None, basis_fun=None):\n",
    "    # Plot the data points and the regression function\n",
    "    plt.subplot()\n",
    "\n",
    "    plt.scatter(data.T, label.T, c=\"blue\", marker=\"x\")\n",
    "\n",
    "    if params:\n",
    "        xmin, xmax = plt.xlim()\n",
    "        x = np.linspace(xmin, xmax, 200)[..., None]\n",
    "        fx = linreg(*params, x, basis_fun)\n",
    "        plt.plot(x, fx, c=\"orange\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35003b04",
   "metadata": {},
   "source": [
    "Implement the `least_squares` function that computes the least-squares solution given some labelled data.\n",
    "Afterwards, complete the `linreg` function; this function should apply a given linear regression model on some data.\n",
    "It optionally transforms the data using a basis function, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(data, label):\n",
    "    # computes the parameters of a least-squares regression model\n",
    "    # Input\n",
    "    #  data  : NxD array of data samples\n",
    "    #  label : Nx1 array of (continuous) targets\n",
    "    # Output\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return weight, bias\n",
    "\n",
    "\n",
    "def linreg(weight, bias, data, basis_fun=None):\n",
    "    # Applies a linear regression model to the given data, optionally with basis function\n",
    "    # Input\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "    #  data   : NxD array of data samples\n",
    "    #  basis_fun : maps an array of samples to an array of feature vectors\n",
    "    # Output\n",
    "    #  predicted label: Nx1 array\n",
    "    #  Be aware that the output.shape should be (N, 1) instead of (N, )!\n",
    "\n",
    "    if basis_fun is not None:\n",
    "        data = basis_fun(data)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f6728",
   "metadata": {},
   "source": [
    "We will need some evaluation measures to judge the performance of our models.\n",
    "Implement the mean-squared error (MSE) and root mean square (RMS) functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, gt):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def rms(pred, gt):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87472a3",
   "metadata": {},
   "source": [
    "Let's apply your implementation on some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"sine\"\n",
    "train_data, train_label = load_data(f\"regression_{dataset_name}_train\")\n",
    "test_data, test_label = load_data(f\"regression_{dataset_name}_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f77a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = least_squares(train_data, train_label)\n",
    "plot_(train_data, train_label, params)\n",
    "plot_(test_data, test_label, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5a5ff",
   "metadata": {},
   "source": [
    "As the example shows, a purely linear model is on its own not able to model complex behavior.\n",
    "However, we can again apply non-linear basis functions to the data.\n",
    "Let's implement some basis function and see what effects we can observe.\n",
    "\n",
    "Implement a polynomial basis function below.\n",
    "Since we work on 1D data here, we can use a simplified formulation:\n",
    "\n",
    "$$\n",
    "\\varphi_d(x) = (1, x, x^2, \\ldots, x^d)^\\mathsf{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2702b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, d):\n",
    "    # Input\n",
    "    #  x : NxD array of data samples (In this case D=1 since input is 1-dimensional)\n",
    "    #  d : degree of the polynomial\n",
    "    # Output\n",
    "    #  feature : Nxd array of transformed features\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86598049",
   "metadata": {},
   "source": [
    "We will now test how well they perform, and see which hyperparameter (i.e. the degree of the polynomial/the maximum frequency of the sinusoid) performs the best.\n",
    "We also plot the norm of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_basis_function(solver, fun, max_degree):\n",
    "    error_train, error_test, weight_norm = [], [], []\n",
    "    for d in range(1, max_degree):\n",
    "        phi = partial(fun, d=d)\n",
    "        params = solver(phi(train_data), train_label)\n",
    "\n",
    "        # uncomment to visualize the regression results for all degrees\n",
    "        plot_(train_data, train_label, params, phi)\n",
    "        plot_(test_data, test_label, params, phi)\n",
    "\n",
    "        train_pred = linreg(*params, train_data, phi)\n",
    "        test_pred = linreg(*params, test_data, phi)\n",
    "\n",
    "        error_train.append(rms(train_pred, train_label))\n",
    "        error_test.append(rms(test_pred, test_label))\n",
    "        weight_norm.append((params[0] ** 2).sum() + params[1] ** 2)\n",
    "    plt.title(fun.__name__)\n",
    "    ln1 = plt.plot(error_train, label=\"Train error\")\n",
    "    ln2 = plt.plot(error_test, label=\"Test error\")\n",
    "    plt.xlabel(\"Degree\"), plt.ylabel(\"RMS\")\n",
    "    ax2 = plt.twinx()\n",
    "    plt.ylabel(\"Norm of w\")\n",
    "    ln3 = plt.plot(weight_norm, label=\"weight norm\", c=\"green\")\n",
    "    plt.legend(ln1 + ln2 + ln3, [\"Train error\", \"Test error\", \"Weight norm\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best test error: {min(error_test)}\")\n",
    "\n",
    "\n",
    "evaluate_basis_function(least_squares, poly, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99bbe9",
   "metadata": {},
   "source": [
    "In addition to the polynomial basis function, we can utilize features based on the Fourier series.\n",
    "In general, we can represent a periodic signal, denoted as $s(x)$, using a series of sines and cosines as follows:\n",
    "\n",
    "$$\n",
    "s(x) = A_0 + \\sum_{n=1}^\\infty(A_n cos(\\frac{2\\pi nx}{P}) + B_n sin(\\frac{2\\pi nx}{P}))\n",
    "$$\n",
    "\n",
    "By utilizing the basis function defined below, we can transform our original time-series data into a combination of sine and cosine waves with different frequencies.\n",
    "Subsequently, the linear regression technique can be applied to estimate the coefficients $A_0$, $A_n$ and $B_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(x, d):\n",
    "    P = 1\n",
    "\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.sin(2 * np.pi * (i // 2) / P * x + (0.5 * np.pi * (i % 2)))\n",
    "            for i in range(2 * d)\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "evaluate_basis_function(least_squares, fourier, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d48c0",
   "metadata": {},
   "source": [
    "While the training loss decreases, the obtained result appears peculiar; in particular, we observe highly oscillating spikes, which do not match our data.\n",
    "Try to find a better fit to the sampled data by adjusting the `P` value in the `fourier` function to produce a more reasonable result.\n",
    "\n",
    "Can you explain why we observe this phenomenon, and why adjusting `P` helps to alleviate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05127cb5",
   "metadata": {},
   "source": [
    "# Q2: Implement regularized least-squares\n",
    "While the results from fitting a regression function with higher-order polynomials look promising, we observed severe overfitting due to the small size of the training set and the large capacity of the model.\n",
    "We will now implement regularization in order to avoid this behavior.\n",
    "\n",
    "Complete the function below to compute the ridge regression solution (i.e. least-squares solution with sqaured penalty on the norm of the weight vector).\n",
    "Keep in mind that we do not want to penalize the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff91118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_least_squares(data, label, lam):\n",
    "    # Input\n",
    "    #  data  : NxD array of data samples\n",
    "    #  label : Nx1 array of (continuous) targets\n",
    "    #  lam   : lambda, the ridge parameter/regularization coefficient\n",
    "    # Output\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93593a",
   "metadata": {},
   "source": [
    "Let's repeat the experiment from above with the regularized least-squares solver.\n",
    "What do you observe now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e357bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in (0.001, 0.01, 0.1):\n",
    "    print(f\"##### lambda = {lam} #####\")\n",
    "    solver = partial(regularized_least_squares, lam=lam)\n",
    "    evaluate_basis_function(solver, poly, 20)\n",
    "    # evaluate_basis_function(solver, fourier, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ef354",
   "metadata": {},
   "source": [
    "# Q3: Extending 1D Regression to 2D\n",
    "In this section, we will apply the algorithm developed in the previous sections to handle 2D data. This means that our `data` will now have a shape of `(N, 2)` (`label` will still have a shape of `(N, 1)`).\n",
    "\n",
    "To demonstrate the concept, we will utilize a 2D Gaussian probability density function (PDF) as our target function. From this target function, we will sample data points to create our input data (x, y) and corresponding labels (z) pairs. Let's visualize the target function to gain a better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt, cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mu, sigma = [0, 0], [[0.1, 0], [0, 0.1]]\n",
    "\n",
    "\n",
    "def plot_gt(ax=None, xlim=[-1, 1], ylim=[-1, 1]):\n",
    "    x, y = np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pos = np.stack([X, Y], axis=-1)\n",
    "    rv1 = multivariate_normal(mu, sigma)\n",
    "    pdf = rv1.pdf(pos)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.axes(projection=\"3d\")\n",
    "    surf = ax.plot_surface(X, Y, pdf, facecolors=cm.viridis(pdf / pdf.max()), alpha=0.2)\n",
    "    surf.set_facecolor((0, 0, 0, 0))\n",
    "\n",
    "\n",
    "plot_gt()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b97763",
   "metadata": {},
   "source": [
    "In this step, we will take some samples from this function and fit a plane to these data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample n_sample points\n",
    "def sample(n_sample, std):\n",
    "\n",
    "    pos = np.random.randn(n_sample, 2) * std\n",
    "\n",
    "    rv1 = multivariate_normal(mu, sigma)\n",
    "    pdf = rv1.pdf(pos) + np.random.randn(n_sample) * 0.1\n",
    "    data, label = pos.reshape(-1, 2), pdf.reshape(-1, 1)\n",
    "    return data, label\n",
    "\n",
    "\n",
    "def plot_result(data, label, weight, bias, fun=None):\n",
    "    n_sample = int(np.sqrt(data.shape[0]))\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "    n_pts = 100\n",
    "    xx = np.linspace(data[:, 0].min(), data[:, 0].max(), n_pts)\n",
    "    yy = np.linspace(data[:, 1].min(), data[:, 1].max(), n_pts)\n",
    "    # plot hypothesis\n",
    "    X, Y = np.meshgrid(xx, yy)\n",
    "    pos = np.c_[X.flatten(), Y.flatten()]\n",
    "    if fun is not None:\n",
    "        pos = fun(pos)\n",
    "    Z = (pos @ weight + bias).reshape(n_pts, n_pts)\n",
    "    ax.plot_surface(X, Y, Z, color=\"yellow\", alpha=0.5)\n",
    "\n",
    "    # plot training data\n",
    "    ax.scatter(data[:, 0], data[:, 1], label, c=\"b\")\n",
    "\n",
    "    # plot ground truth\n",
    "    plot_gt(\n",
    "        ax,\n",
    "        xlim=[data[:, 0].min(), data[:, 0].max()],\n",
    "        ylim=[data[:, 1].min(), data[:, 1].max()],\n",
    "    )\n",
    "\n",
    "    ax.axes.set_xlim3d(left=data[:, 0].min(), right=data[:, 0].max())\n",
    "    ax.axes.set_ylim3d(bottom=data[:, 1].min(), top=data[:, 1].max())\n",
    "    ax.axes.set_zlim3d(bottom=label.min(), top=label.max())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_sample = 100\n",
    "train_data, train_label = sample(n_sample, std=0.4)\n",
    "test_data, test_label = sample(n_sample * 2, std=0.3)\n",
    "\n",
    "weight, bias = least_squares(train_data, train_label)\n",
    "label_pred = linreg(weight, bias, train_data)\n",
    "\n",
    "print(\"Train mean squared error :\", mse(label_pred, train_label))\n",
    "print(\"Test mean squared error :\", mse(linreg(weight, bias, test_data), test_label))\n",
    "plot_result(train_data, train_label, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddce4a",
   "metadata": {},
   "source": [
    "As expected, a plane is unable to properly fit the given data points. Therefore, it becomes necessary to employ a non-linear basis function to transform the data.\n",
    "\n",
    "Implement a 2D polynomial basis function `poly2d` in the code cell below.\n",
    "Use the formula\n",
    "\n",
    "$$ \n",
    "\\varphi _d(x, y)_{i,j} = ( x^i y^j ) \\text{ for } i = 0,...,d, j = 0, ..., d-i \\\\\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "$\n",
    "\\varphi _1(\\mathbf{x}): ( x, y ) \\mapsto (1, x, y) \\\\\n",
    "\\varphi _2(\\mathbf{x}): ( x, y ) \\mapsto (1, x, x^2, y, xy, ) \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2d(x, d):\n",
    "    # Input\n",
    "    #  x : NxD array of data samples (In this case D=2 since input is 2-dimensional)\n",
    "    #  d : degree of the polynomial\n",
    "\n",
    "    # Output\n",
    "    #  feature : N x Dout array of transformed features,\n",
    "    #            where Dout = (d+1)(d+2)/2\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    feature = np.stack(\n",
    "        [x[:, 0] ** i * x[:, 1] ** j for i in range(d + 1) for j in range(d + 1 - i)],\n",
    "        axis=-1,\n",
    "    )\n",
    "    return feature\n",
    "\n",
    "\n",
    "for d in [1, 2, 3, 4]:\n",
    "    weight, bias = least_squares(poly2d(train_data, d), train_label)\n",
    "    label_pred = linreg(weight, bias, poly2d(train_data, d))\n",
    "\n",
    "    print(\"Degree :\", d)\n",
    "    print(\"Train mean squared error :\", mse(label_pred, train_label))\n",
    "    print(\n",
    "        \"Test mean squared error :\",\n",
    "        mse(linreg(weight, bias, poly2d(test_data, d)), test_label),\n",
    "    )\n",
    "    plot_result(train_data, train_label, weight, bias, partial(poly2d, d=d))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bccf68",
   "metadata": {},
   "source": [
    "# Q4: Error Decompositions in the Bias-Variance Tradeoff\n",
    "In this exercise, you will derive the error decomposition that we used to understand the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bf71d",
   "metadata": {},
   "source": [
    "Derive the error decomposition of the squared error into structural error and noise:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[L]=\\int\\{y(\\mathbf{x})-\\mathbb{E}[t \\mid \\mathbf{x}]\\}^2 p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\{\\mathbb{E}[t \\mid \\mathbf{x}]-t\\}^2 p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
